{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Alex/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import the relevand section of the NLP library\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "import re\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seinfeld_directory = 'Seinfeld_Episodes/'\n",
    "\n",
    "seinfeld_season_1_episodes = ['Season_1/S01_E01_The_Seinfeld_Chronicles.txt', 'Season_1/S01_E02_The_Robbery.txt',\n",
    "                             'Season_1/S01_E03_Male_Unbonding.txt', 'Season_1/S01_E04_The_Stock_Tip.txt']\n",
    "\n",
    "seinfeld_season_5_episodes = ['Season_5/S05_E01_The_Mango.txt', 'Season_5/S05_E02_The_Puffy_Shirt.txt',\n",
    "                              'Season_5/S05_E03_The_Glasses.txt', 'Season_5/S05_E04_The_Sniffing_Accountant.txt',\n",
    "                              'Season_5/S05_E05_The_Bris.txt', 'Season_5/S05_E06_The_Lip_Reader.txt',\n",
    "                              'Season_5/S05_E07_The_Non_Fat_Yogurt.txt', 'Season_5/S05_E08_The_Barber.txt',\n",
    "                              'Season_5/S05_E09_The_Masseuse.txt', 'Season_5/S05_E10_The_Cigar_Store_Indian.txt',\n",
    "                              'Season_5/S05_E11_The_Conversion.txt', 'Season_5/S05_E12_The_Stall.txt',\n",
    "                              'Season_5/S05_E13_The_Dinner_Party.txt', 'Season_5/S05_E14_The_Marine_Biologist.txt',\n",
    "                              'Season_5/S05_E15_The_Pie.txt', 'Season_5/S05_E16_The_Stand-In.txt',\n",
    "                              'Season_5/S05_E17_The_Wife.txt', 'Season_5/S05_E18_The_Raincoats_Part_1.txt',\n",
    "                              'Season_5/S05_E19_The_Raincoats_Part_2.txt', 'Season_5/S05_E20_The_Fire.txt',\n",
    "                              'Season_5/S05_E21_The_Hamptons.txt', 'Season_5/S05_E22_The Opposite.txt']\n",
    "\n",
    "curb_directory = 'Curb_Episodes/'\n",
    "\n",
    "curb_season_3_episodes = [\"Season_3/S03_E01_Chet's_Shirt.txt\", 'Season_3/S03_E02_The_Benadryl_Brownie.txt',\n",
    "                              'Season_3/S03_E03_Club_Soda_and_Salt .txt', 'Season_3/S03_E04_The_Nanny_from_Hell.txt',\n",
    "                              'Season_3/S03_E05_The_Terrorist_Attack.txt', 'Season_3/S03_E06_The_Special_Section.txt',\n",
    "                              'Season_3/S03_E07_The_Corpse-Sniffing_Dog.txt', 'Season_3/S03_E08_Krazee-Eyez_Killa.txt',\n",
    "                              'Season_3/S03_E09_Mary,_Joseph_and_Larry.txt', 'Season_3/S03_E10_The_Grand_Opening.txt']\n",
    "\n",
    "curb_season_4_episodes = [\"Season_4/S04_E01_Mel's_Offer.txt\", \"Season_4/S04_E02_Ben's_Birthday_Party.txt\",\n",
    "                         'Season_4/S04_E03_The_Blind_Date.txt', 'Season_4/S04_E04_The_Weatherman.txt',\n",
    "                         'Season_4/S04_E05_The_5_Wood.txt', 'Season_4/S04_E06_The_Car_Pool_Lane.txt',\n",
    "                         'Season_4/S04_E07_The_Surrogate.txt', 'Season_4/S04_E08_Wandering_Bear.txt',\n",
    "                         'Season_4/S04_E09_The_Survivor.txt', 'Season_4/S04_E10_Opening_Night.txt']\n",
    "\n",
    "curb_season_7_episodes = [\"Season_7/S07_E01_Funkhouser's_Crazy_Sister.txt\", 'Season_7/S07_E02_Vehicular_Fellatio.txt',\n",
    "                         'Season_7/S07_E03_The_Reunion.txt', 'Season_7/S07_E04_The_Hot_Towel.txt',\n",
    "                         'Season_7/S07_E05_Denise_Handicapped.txt', 'Season_7/S07_E06_The_Bare_Midriff.txt',\n",
    "                         'Season_7/S07_E07_The_Black_Swan.txt', 'Season_7/S07_E08_Officer_Krupke.txt',\n",
    "                         'Season_7/S07_E09_The_Table_Read.txt', 'Season_7/S07_E10_Seinfeld.txt']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file_names_to_dictionary(directory_name, list_of_file_names):\n",
    "\n",
    "    episode_info = []\n",
    "    \n",
    "    show_name = \" \".join(directory_name.split('/')[0].split('_')[:-1])\n",
    "    \n",
    "    for i in list_of_file_names:\n",
    "        \n",
    "        season = int(\" \".join(i.split('/')[0].split('_')[1:]))\n",
    "        \n",
    "        i_contents = i.split('/')[1].split('_')\n",
    "        \n",
    "        episode = int(i_contents[1][1:])\n",
    "        \n",
    "        episode_name = \" \".join(i_contents[2:])[:-4]\n",
    "        \n",
    "        episode_info.append({'show_name': show_name, 'season': season, 'episode': episode, 'episode_name': episode_name, 'file_path': directory_name + i})\n",
    "        \n",
    "    return episode_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'show_name': 'Seinfeld',\n",
       "  'season': 1,\n",
       "  'episode': 1,\n",
       "  'episode_name': 'The Seinfeld Chronicles',\n",
       "  'file_path': 'Seinfeld_Episodes/Season_1/S01_E01_The_Seinfeld_Chronicles.txt'},\n",
       " {'show_name': 'Seinfeld',\n",
       "  'season': 1,\n",
       "  'episode': 2,\n",
       "  'episode_name': 'The Robbery',\n",
       "  'file_path': 'Seinfeld_Episodes/Season_1/S01_E02_The_Robbery.txt'},\n",
       " {'show_name': 'Seinfeld',\n",
       "  'season': 1,\n",
       "  'episode': 3,\n",
       "  'episode_name': 'Male Unbonding',\n",
       "  'file_path': 'Seinfeld_Episodes/Season_1/S01_E03_Male_Unbonding.txt'},\n",
       " {'show_name': 'Seinfeld',\n",
       "  'season': 1,\n",
       "  'episode': 4,\n",
       "  'episode_name': 'The Stock Tip',\n",
       "  'file_path': 'Seinfeld_Episodes/Season_1/S01_E04_The_Stock_Tip.txt'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_file_names_to_dictionary(seinfeld_directory, seinfeld_season_1_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seinfeld_dict_season_1 = convert_file_names_to_dictionary(seinfeld_directory, seinfeld_season_1_episodes)\n",
    "\n",
    "seinfeld_dict_season_5 = convert_file_names_to_dictionary(seinfeld_directory, seinfeld_season_5_episodes)\n",
    "\n",
    "curb_dict_season_3 = convert_file_names_to_dictionary(curb_directory, curb_season_3_episodes)\n",
    "\n",
    "curb_dict_season_4 = convert_file_names_to_dictionary(curb_directory, curb_season_4_episodes)\n",
    "\n",
    "curb_dict_season_7 = convert_file_names_to_dictionary(curb_directory, curb_season_7_episodes)\n",
    "\n",
    "shows_dict = seinfeld_dict_season_1 + seinfeld_dict_season_5 + curb_dict_season_3 + curb_dict_season_4 + curb_dict_season_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>file_path</th>\n",
       "      <th>season</th>\n",
       "      <th>show_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Seinfeld Chronicles</td>\n",
       "      <td>Seinfeld_Episodes/Season_1/S01_E01_The_Seinfel...</td>\n",
       "      <td>1</td>\n",
       "      <td>Seinfeld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The Robbery</td>\n",
       "      <td>Seinfeld_Episodes/Season_1/S01_E02_The_Robbery...</td>\n",
       "      <td>1</td>\n",
       "      <td>Seinfeld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Male Unbonding</td>\n",
       "      <td>Seinfeld_Episodes/Season_1/S01_E03_Male_Unbond...</td>\n",
       "      <td>1</td>\n",
       "      <td>Seinfeld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The Stock Tip</td>\n",
       "      <td>Seinfeld_Episodes/Season_1/S01_E04_The_Stock_T...</td>\n",
       "      <td>1</td>\n",
       "      <td>Seinfeld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The Mango</td>\n",
       "      <td>Seinfeld_Episodes/Season_5/S05_E01_The_Mango.txt</td>\n",
       "      <td>5</td>\n",
       "      <td>Seinfeld</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode             episode_name  \\\n",
       "0        1  The Seinfeld Chronicles   \n",
       "1        2              The Robbery   \n",
       "2        3           Male Unbonding   \n",
       "3        4            The Stock Tip   \n",
       "4        1                The Mango   \n",
       "\n",
       "                                           file_path  season show_name  \n",
       "0  Seinfeld_Episodes/Season_1/S01_E01_The_Seinfel...       1  Seinfeld  \n",
       "1  Seinfeld_Episodes/Season_1/S01_E02_The_Robbery...       1  Seinfeld  \n",
       "2  Seinfeld_Episodes/Season_1/S01_E03_Male_Unbond...       1  Seinfeld  \n",
       "3  Seinfeld_Episodes/Season_1/S01_E04_The_Stock_T...       1  Seinfeld  \n",
       "4   Seinfeld_Episodes/Season_5/S05_E01_The_Mango.txt       5  Seinfeld  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(shows_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(file_path):\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        raw_text = file.read().replace('\\n', ' ')\n",
    "    \n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_episode(raw_text):\n",
    "    \n",
    "    raw_text_no_notes = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", raw_text)\n",
    "\n",
    "    for symbol in \"*,#-.?!''\\n\":\n",
    "        raw_text_no_notes = raw_text_no_notes.replace(symbol, '').lower()\n",
    "  \n",
    "    cleaned_text = raw_text_no_notes.split(\" \")    \n",
    "    \n",
    "    for i in cleaned_text:\n",
    "        \n",
    "        if i.endswith(':') == True or i == '' or i == ' ':\n",
    "            cleaned_text.remove(i)\n",
    "            \n",
    "        i = i.replace('.', '')\n",
    "        i = i.replace('?', '')\n",
    "        i = i.replace('!', '')\n",
    "\n",
    "     \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['so', 'im', 'on', 'line', 'at', 'the', 'supermarket', 'two', 'women', 'in', 'front', 'of', 'me', 'one', 'of', 'them', 'her', 'total', 'was', 'eight']\n",
      "['so', 'i', 'move', 'into', 'the', 'centre', 'lane', 'now', 'i', 'get', 'ahead', 'of', 'this', 'women', 'who', 'felt', 'for', 'some', 'reason', 'i']\n",
      "['most', 'men', 'like', 'working', 'on', 'things', 'tools', 'objects', 'fixing', 'things', 'this', 'is', 'what', 'men', 'enjoy', 'doing', 'have', 'you', 'ever', 'noticed']\n",
      "['went', 'out', 'to', 'dinner', 'the', 'other', 'night', 'check', 'came', 'at', 'the', 'end', 'of', 'the', 'meal', 'as', 'it', 'always', 'does', 'never']\n",
      "['a', 'female', 'orgasm', 'is', 'kinda', 'like', 'the', 'bat', 'cave', 'a', 'very', 'few', 'people', 'know', 'where', 'it', 'is', 'and', 'if', 'youre']\n",
      "['i', 'cant', 'believe', 'this', 'oh', 'it', 'wont', 'be', 'for', 'that', 'long', 'how', 'can', 'i', 'do', 'this', 'how', 'can', 'i', 'move']\n",
      "['i', 'never', 'get', 'enough', 'sleep', 'i', 'stay', 'up', 'late', 'at', 'night', 'cause', 'im', 'night', 'guy', 'night', 'guy', 'wants', 'to', 'stay']\n",
      "['jerry', 'george', 'and', 'elaine', 'at', 'monks', 'caf', 'so', 'does', 'he', 'like', 'you', 'what', 'do', 'you', 'think', 'you', 'like', 'him', 'yeah']\n",
      "['act', 'one', 'scene', 'a', 'int', 'hospital', 'room', 'day', 'jerry', 'elaine', 'and', 'george', 'are', 'visiting', 'with', 'new', 'parents', 'myra', 'and', 'stan']\n",
      "['at', 'the', 'comedy', 'club', 'professional', 'tennis', 'to', 'me', 'i', 'dont', 'understand', 'all', 'the', 'shushing', 'why', 'are', 'they', 'always', 'shushing', 'shh']\n",
      "['jerry', 'ive', 'always', 'been', 'a', 'big', 'fan', 'of', 'the', 'little', 'check', 'move', 'you', 'know', 'unless', 'the', 'waiters', 'not', 'too', 'sharp']\n",
      "['mr', 'well', 'george', 'we', 'here', 'at', 'sanalac', 'like', 'to', 'think', 'of', 'ourselves', 'as', 'a', 'fairly', 'progressive', 'company', 'we', 'have', 'a']\n",
      "['if', 'theres', 'a', 'serial', 'killer', 'lose', 'in', 'your', 'neighborhood', 'it', 'seems', 'like', 'the', 'safest', 'thing', 'is', 'to', 'be', 'the', 'neighbor']\n",
      "['jerrys', 'standup', 'piece', 'you', 'can', 'always', 'tell', 'what', 'was', 'the', 'best', 'year', 'of', 'your', 'fathers', 'life', 'because', 'they', 'seem', 'to']\n",
      "['at', 'the', 'comedy', 'club', 'you', 'know', 'doctor', 'is', 'supposed', 'to', 'be', 'such', 'a', 'prestigious', 'occupation', 'but', 'its', 'really', 'like', 'one']\n",
      "['elaine', 'what', 'a', 'dope', 'uhexcuse', 'me', 'umm', 'im', 'sorry', 'this', 'is', 'this', 'is', 'kind', 'of', 'embarrassing', 'but', 'theres', 'no', 'toilet']\n",
      "['monologue', 'we', 'never', 'should', 'have', 'landed', 'a', 'man', 'on', 'the', 'moon', 'its', 'a', 'mistake', 'now', 'everything', 'is', 'compared', 'to', 'that']\n",
      "['well', 'did', 'he', 'bring', 'it', 'up', 'in', 'the', 'meeting', 'elaine', 'see', 'this', 'tshirts', 'six', 'years', 'ive', 'had', 'this', 'tshirts', 'its']\n",
      "['hum', 'that', 'was', 'really', 'good', 'yeah', 'are', 'you', 'full', 'oh', 'no', 'ive', 'had', 'just', 'enough', 'here', 'we', 'go', 'apple', 'pie']\n",
      "['opening', 'the', 'bus', 'is', 'the', 'single', 'stupidest', 'fattest', 'slowest', 'most', 'despised', 'vehicle', 'on', 'the', 'road', 'isnt', 'it', 'you', 'ever', 'notice']\n",
      "['jerrys', 'of', 'all', 'the', 'places', 'that', 'you', 'go', 'all', 'the', 'time', 'the', 'drycleaning', 'relationship', 'is', 'one', 'of', 'the', 'most', 'bizarre']\n",
      "['i', 'really', 'feel', 'as', 'human', 'beings', 'we', 'need', 'more', 'training', 'in', 'our', 'basic', 'social', 'skills', 'conversational', 'distance', 'dont', 'you', 'hate']\n",
      "['i', 'really', 'feel', 'as', 'human', 'beings', 'we', 'need', 'more', 'training', 'in', 'our', 'basic', 'social', 'skills', 'conversational', 'distance', 'dont', 'you', 'hate']\n",
      "['jerrys', 'to', 'me', 'the', 'thing', 'about', 'birthday', 'parties', 'is', 'that', 'the', 'first', 'birthday', 'party', 'you', 'have', 'and', 'the', 'last', 'birthday']\n",
      "['opening', 'look', 'at', 'the', 'work', 'people', 'do', 'to', 'get', 'to', 'the', 'ocean', 'theyll', 'fight', 'the', 'traffic', 'and', 'the', 'heat', 'and']\n",
      "['opening', 'monologue', 'jerry', 'it', 'is', 'pretty', 'hard', 'to', 'justify', 'at', 'this', 'point', 'in', 'history', 'the', 'existence', 'of', 'men', 'and', 'their']\n",
      "['so', 'ive', 'given', 'up', 'red', 'meat', 'really', 'yep', 'no', 'more', 'red', 'meat', 'for', 'me', 'good', 'for', 'you', 'thats', 'it', 'im']\n",
      "['you', 'know', 'i', 'found', 'my', 'soul', 'mate', 'not', 'only', 'do', 'i', 'think', 'im', 'in', 'love', 'with', 'her', 'but', 'i', 'think']\n",
      "['my', 'agent', 'calls', 'he', 'has', 'an', 'audition', 'for', 'me', 'for', 'a', 'biker', 'right', 'so', 'im', 'just', 'psyched', 'and', 'i', 'get']\n",
      "['my', 'this', 'is', 'really', 'coming', 'along', 'here', 'hey', 'ill', 'take', 'a', 'seven', 'and', 'seven', 'what', 'are', 'you', 'doing', 'a', 'little']\n",
      "['four', 'for', 'david', 'let', 'me', 'see', 'no', 'im', 'not', 'finding', 'that', 'try', 'braudy', 'braudy', 'here', 'we', 'go', 'for', 'four', 'all']\n",
      "['didnt', 'think', 'i', 'was', 'gonna', 'find', 'out', 'huh', 'is', 'that', 'what', 'you', 'thought', 'if', 'you', 'were', 'me', 'you', 'wouldnt', 'have']\n",
      "['larry', 'david', 'hi', 'phil', 'dunlap', 'here', 'for', 'the', 'chefs', 'position', 'oh', 'hi', 'hi', 'thank', 'you', 'thank', 'you', 'very', 'much', 'yeah']\n",
      "['whoo', 'not', 'bad', 'huh', 'thats', 'not', 'bad', 'at', 'all', 'thats', 'official', 'know', 'what', 'i', 'mean', 'yeah', 'did', 'you', 'pick', 'that']\n",
      "['hey', 'larry', 'hey', 'how', 'are', 'you', 'okay', 'good', 'hey', 'listen', 'is', 'this', 'true', 'did', 'i', 'hear', 'that', 'you', 'were', 'at']\n",
      "['its', 'something', 'a', 'kentucky', 'farmer', 'says', 'shes', 'never', 'seen', 'before', 'a', 'hen', 'taking', 'newborn', 'puppies', 'under', 'her', 'wing', 'so', 'guess']\n",
      "['pasta', 'if', 'you', 'like', 'uh', 'very', 'good', 'what', 'because', 'of', 'what', 'eh', 'what', 'whats', 'the', 'sauce', 'with', 'the', 'fish', 'what']\n",
      "['thank', 'god', 'you', 'stopped', 'okay', 'um', 'im', 'not', 'even', 'gonna', 'get', 'into', 'whats', 'wrong', 'with', 'the', 'actual', 'choreography', 'im', 'gonna']\n",
      "['you', 'know', 'you', 'should', 'get', 'one', 'of', 'these', 'cordless', 'vacuums', 'hey', 'mikey', 'boy', 'what', 'what', 'im', 'talking', 'yeah', 'sorry', 'i']\n",
      "['there', 'there', 'we', 'go', 'larry', 'let', 'me', 'get', 'this', 'cotton', 'out', 'for', 'you', 'and', 'youll', 'be', 'ready', 'for', 'dr', 'funkhouser']\n",
      "['leo', 'and', 'max', 'up', 'off', 'our', 'backs', 'back', 'on', 'the', 'great', 'white', 'way', 'leo', 'and', 'max', 'back', 'on', 'the', 'tracks']\n",
      "['a', 'beautiful', 'sunday', 'were', 'sitting', 'here', 'like', 'idiots', 'we', 'should', 'be', 'playing', 'golf', 'yeah', 'how', 'do', 'you', 'work', 'a', 'glue']\n",
      "['puffy', 'shirt', 'puffy', 'shirt', 'can', 'you', 'spare', 'a', 'little', 'change', 'for', 'an', 'old', 'buccaneer', 'you', 'know', 'its', 'really', 'not', 'a']\n",
      "['what', 'does', 'it', 'take', 'to', 'be', 'a', '\"girls', 'gone', 'wild\"', 'girl', 'you', 'have', 'to', 'show', 'your', 'those', 'girls', 'from', 'cancun']\n",
      "['i', 'dont', 'know', 'why', 'i', 'ever', 'married', 'this', 'man', 'what', 'a', 'schmuck', 'shlomo', 'what', 'is', 'taking', 'you', 'so', 'long', 'my']\n",
      "['who', 'are', 'you', 'lm', 'leopold', 'bloom', 'lm', 'an', 'accountant', 'lm', 'from', 'whitehall', '&', 'marks', 'lve', 'come', 'here', 'to', 'do', 'your']\n",
      "['finally', 'thank', 'you', 'youre', 'welcome', 'no', 'good', 'larry', 'it', 'tastes', 'like', 'you', 'put', 'the', 'whole', 'salt', 'shaker', 'up', 'in', 'here']\n",
      "['hmm', 'er', 'ah', 'hey', 'ld', 'what', 'i', 'need', 'you', 'upstairs', 'now', 'yo', 'ld', 'where', 'you', 'at', 'ld', 'ld', 'what', 'channel']\n",
      "['so', 'loretta', 'was', 'swearing', 'at', 'you', 'and', 'yelling', 'at', 'you', 'yeah', 'and', 'she', 'really', 'thought', 'you', 'were', 'cheating', 'on', 'her']\n",
      "['let', 'me', 'uh', 'let', 'me', 'give', 'you', 'a', 'little', 'tip', 'okay', 'mmhmm', 'for', 'traveling', 'a', 'traveling', 'tip', 'try', 'not', 'to']\n",
      "['sorry', 'im', 'sorry', 'sorry', 'who', 'are', 'you', 'listening', 'to', 'what', 'who', 'are', 'you', 'listening', 'to', 'um', 'cheeyun', 'oh', 'im', 'a']\n",
      "['jeez', 'what', 'are', 'you', 'doing', 'in', 'there', 'what', 'in', 'the', 'world', 'was', 'that', 'what', 'what', 'do', 'you', 'got', 'seabiscuit', 'in']\n",
      "['god', 'only', 'knows', 'what', 'that', 'man', 'has', 'let', 'go', 'through', 'lucky', 'he', 'didnt', 'put', '\"beloved', 'moth', '\"', 'hey', 'hey', 'hey']\n",
      "['if', 'sammi', 'doesnt', 'want', 'to', 'take', 'clarinet', 'anymore', 'she', 'doesnt', 'want', 'to', 'i', 'dont', 'wanna', 'push', 'her', 'youre', 'the', 'one']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uhhuh', 'unbelievable', 'right', 'is', 'this', 'insane', 'larry', 'its', 'exactly', 'the', 'same', 'exactly', 'the', 'same', 'yeah', 'crazy', 'i', 'mean', 'my', 'gosh']\n",
      "['ive', 'been', 'dreading', 'telling', 'you', 'about', 'this', 'whole', 'madoff', 'thing', 'oh', 'well', 'you', 'must', 'hate', 'me', 'for', 'losing', 'all', 'our']\n"
     ]
    }
   ],
   "source": [
    "for i in df.file_path:\n",
    "    episode_text = open_file(i)\n",
    "    print(cleaned_episode(episode_text)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Two: tokenize/stemming/lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Jerry: So, I'm on line at the supermarket. Two women in front of me. One of  them, her total was eight dollars, the other three dollars. They both  of course choose to pay by the use of the (pause and gesture to audience  for response)  Audience: Cheque  Jerry: Cheque. Now, the fact is, if it's a woman in front of you that's  writing the cheque, you will not be waiting long. I have noticed that  women are very *fast* with cheques, y'know, 'cuz they write out so many  cheques. The keys, they can \""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_1_text = open_file(df.loc[0].file_path)\n",
    "episode_1_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'so im on line at the supermarket two women in front of me one of them her total was eight dollars the other three dollars they both of course choose to pay by the use of the cheque cheque now the fact is if its a woman in front of you thats writing the cheque you will not be waiting long i have noticed that women are very fast with cheques yknow cuz they write out so many cheques the keys they can never find in their purse they dont know where that is but the cheque book they got that they never fumble for the cheque book the cheque book comes out of a ``who do i make it out to theres my id theres something about a cheque that to a man is not masculine i dont know exactly what it is i think to a man a cheque is like a note from your mother that says ``i dont have any money but if youll contact these people im sure theyll stick up for me if you just trust me this one time i dont have any money but i have these i wrote on these; is this of any value at all whats that one \"coccoon the return\" i guess they didnt like it up there maybe they came back for chinese food yknow maureen stapleton if she gets a craving shes probably screamin at those aliens \"i gotta have a lo mein\" okay whatre we doing here i have seen everything oh yeah i dont believe youve seen this % jerry shows elaine some movie from the adult section thats probably % called \"the sperminator\" or something elaine: oh lovely yeah elaine: what do you think their parents think jerry: \"so uh whats your son doing now dr stevens\" \"oh hes a public fornicator yes hes a fine boy\" elaine: yknow what this would be a really funny gift for pamelas birthday jerry: pamela do i know her elaine: yeah you met her when we were going out jerry: oh yeah right elaine: you have no idea who im talking about do you jerry: no elaine: blonde hair remember glasses have you totally blocked out the entire time we were a couple % bing jerry: riverside drive elaine: right in fact no never mind jerry: well what is it elaine: well a bunch of people are getting together tomorrow night at some bar for her birthday but you dont want to go to that no jerry: wait a second wait a second we could work out a little deal here elaine: what little deal jerry: i will go to that if you go with me to a little family wedding i have on saturday elaine: a wedding have you lost it man jerry: yknow my parents are coming in for this elaine: theyre coming in jerry: yeah tomorrow elaine: hey did your father ever get that hair weave jerry: no no still doin the big'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_episode_1_text = cleaned_episode(episode_1_text)\n",
    "\" \".join(cleaned_episode_1_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(clean_text):\n",
    "    joined_episode = ' '.join(clean_text)\n",
    "    tokenized_episode = word_tokenize(joined_episode)\n",
    "    \n",
    "    return tokenized_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"so im on line at the supermarket two women in front of me one of them her total was eight dollars the other three dollars they both of course choose to pay by the use of the cheque cheque now the fact is if its a woman in front of you thats writing the cheque you will not be waiting long i have noticed that women are very fast with cheques yknow cuz they write out so many cheques the keys they can never find in their purse they dont know where that is but the cheque book they got that they never fumble for the cheque book the cheque book comes out of a `` who do i make it out to theres my id theres something about a cheque that to a man is not masculine i dont know exactly what it is i think to a man a cheque is like a note from your mother that says `` i dont have any money but if youll contact these people im sure theyll stick up for me if you just trust me this one time i dont have any money but i have these i wrote on these ; is this of any value at all whats that one `` coccoon the return '' i guess they didnt like it up there maybe they came back for chinese food yknow maureen stapleton if she gets a craving shes probably screamin at those aliens `` i got ta have a lo mein '' okay whatre we doing here i have seen everything oh yeah i dont believe youve seen this % jerry shows elaine some movie from the adult section thats probably % called `` the sperminator '' or something elaine : oh lovely yeah elaine : what do you think their parents think jerry : `` so uh whats your son doing now dr stevens '' `` oh hes a public fornicator yes hes a fine boy '' elaine : yknow what this would be a really funny gift for pamelas birthday jerry : pamela do i know her elaine : yeah you met her when we were going out jerry : oh yeah right elaine : you have no idea who im talking about do you jerry : no elaine : blonde hair remember glasses have you totally blocked out the entire time we were a couple % bing jerry : riverside drive elaine : right in fact no never mind jerry : well what is it elaine : well a bunch of people are getting together tomorrow night at some bar for her birthday but you dont want to go to that no jerry : wait a second wait a second we could work out a little deal here elaine : what little deal jerry : i will go to that if you go with me to a little family wedding i have on saturday elaine : a wedding have you lost it man jerry\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_episode_1_text = tokenize(cleaned_episode_1_text)\n",
    "\" \".join(tokenize_episode_1_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(tokenized_text):\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in tokenized_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Function To Process Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(file_name):\n",
    "    raw_episode_text = open_file(file_name)\n",
    "    clean_episode_text = cleaned_episode(raw_episode_text)\n",
    "    tokenize_episode_text = tokenize(clean_episode_text)\n",
    "    lemmatize_episode_text = lemmatize_text(tokenize_episode_text)\n",
    "    \n",
    "    return lemmatize_episode_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['lemmatize_text'] = df.file_path.apply(lambda x: process_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>file_path</th>\n",
       "      <th>season</th>\n",
       "      <th>show_name</th>\n",
       "      <th>lemmatize_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Seinfeld Chronicles</td>\n",
       "      <td>Seinfeld_Episodes/Season_1/S01_E01_The_Seinfel...</td>\n",
       "      <td>1</td>\n",
       "      <td>Seinfeld</td>\n",
       "      <td>so im on line at the supermarket two woman in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The Robbery</td>\n",
       "      <td>Seinfeld_Episodes/Season_1/S01_E02_The_Robbery...</td>\n",
       "      <td>1</td>\n",
       "      <td>Seinfeld</td>\n",
       "      <td>so i move into the centre lane now i get ahead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Male Unbonding</td>\n",
       "      <td>Seinfeld_Episodes/Season_1/S01_E03_Male_Unbond...</td>\n",
       "      <td>1</td>\n",
       "      <td>Seinfeld</td>\n",
       "      <td>most men like working on thing tool object fix...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The Stock Tip</td>\n",
       "      <td>Seinfeld_Episodes/Season_1/S01_E04_The_Stock_T...</td>\n",
       "      <td>1</td>\n",
       "      <td>Seinfeld</td>\n",
       "      <td>went out to dinner the other night check came ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The Mango</td>\n",
       "      <td>Seinfeld_Episodes/Season_5/S05_E01_The_Mango.txt</td>\n",
       "      <td>5</td>\n",
       "      <td>Seinfeld</td>\n",
       "      <td>a female orgasm is kinda like the bat cave a v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode             episode_name  \\\n",
       "0        1  The Seinfeld Chronicles   \n",
       "1        2              The Robbery   \n",
       "2        3           Male Unbonding   \n",
       "3        4            The Stock Tip   \n",
       "4        1                The Mango   \n",
       "\n",
       "                                           file_path  season show_name  \\\n",
       "0  Seinfeld_Episodes/Season_1/S01_E01_The_Seinfel...       1  Seinfeld   \n",
       "1  Seinfeld_Episodes/Season_1/S01_E02_The_Robbery...       1  Seinfeld   \n",
       "2  Seinfeld_Episodes/Season_1/S01_E03_Male_Unbond...       1  Seinfeld   \n",
       "3  Seinfeld_Episodes/Season_1/S01_E04_The_Stock_T...       1  Seinfeld   \n",
       "4   Seinfeld_Episodes/Season_5/S05_E01_The_Mango.txt       5  Seinfeld   \n",
       "\n",
       "                                      lemmatize_text  \n",
       "0  so im on line at the supermarket two woman in ...  \n",
       "1  so i move into the centre lane now i get ahead...  \n",
       "2  most men like working on thing tool object fix...  \n",
       "3  went out to dinner the other night check came ...  \n",
       "4  a female orgasm is kinda like the bat cave a v...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>file_path</th>\n",
       "      <th>season</th>\n",
       "      <th>show_name</th>\n",
       "      <th>lemmatize_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>6</td>\n",
       "      <td>The Bare Midriff</td>\n",
       "      <td>Curb_Episodes/Season_7/S07_E06_The_Bare_Midrif...</td>\n",
       "      <td>7</td>\n",
       "      <td>Curb</td>\n",
       "      <td>jeez what are you doing in there what in the w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>7</td>\n",
       "      <td>The Black Swan</td>\n",
       "      <td>Curb_Episodes/Season_7/S07_E07_The_Black_Swan.txt</td>\n",
       "      <td>7</td>\n",
       "      <td>Curb</td>\n",
       "      <td>god only know what that man ha let go through ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>8</td>\n",
       "      <td>Officer Krupke</td>\n",
       "      <td>Curb_Episodes/Season_7/S07_E08_Officer_Krupke.txt</td>\n",
       "      <td>7</td>\n",
       "      <td>Curb</td>\n",
       "      <td>if sammi doesnt want to take clarinet anymore ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>9</td>\n",
       "      <td>The Table Read</td>\n",
       "      <td>Curb_Episodes/Season_7/S07_E09_The_Table_Read.txt</td>\n",
       "      <td>7</td>\n",
       "      <td>Curb</td>\n",
       "      <td>uhhuh unbelievable right is this insane larry ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>10</td>\n",
       "      <td>Seinfeld</td>\n",
       "      <td>Curb_Episodes/Season_7/S07_E10_Seinfeld.txt</td>\n",
       "      <td>7</td>\n",
       "      <td>Curb</td>\n",
       "      <td>ive been dreading telling you about this whole...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    episode      episode_name  \\\n",
       "51        6  The Bare Midriff   \n",
       "52        7    The Black Swan   \n",
       "53        8    Officer Krupke   \n",
       "54        9    The Table Read   \n",
       "55       10          Seinfeld   \n",
       "\n",
       "                                            file_path  season show_name  \\\n",
       "51  Curb_Episodes/Season_7/S07_E06_The_Bare_Midrif...       7      Curb   \n",
       "52  Curb_Episodes/Season_7/S07_E07_The_Black_Swan.txt       7      Curb   \n",
       "53  Curb_Episodes/Season_7/S07_E08_Officer_Krupke.txt       7      Curb   \n",
       "54  Curb_Episodes/Season_7/S07_E09_The_Table_Read.txt       7      Curb   \n",
       "55        Curb_Episodes/Season_7/S07_E10_Seinfeld.txt       7      Curb   \n",
       "\n",
       "                                       lemmatize_text  \n",
       "51  jeez what are you doing in there what in the w...  \n",
       "52  god only know what that man ha let go through ...  \n",
       "53  if sammi doesnt want to take clarinet anymore ...  \n",
       "54  uhhuh unbelievable right is this insane larry ...  \n",
       "55  ive been dreading telling you about this whole...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Three: Vectorizing the Text & Creating The Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Seinfeld', 'Curb'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unique target values\n",
    "df.show_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encode the Target Variable show_name\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "le.fit(df.show_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show_name = le.transform(df.show_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run to undo the label encoder\n",
    "#df.show_name = le.inverse_transform(df.show_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>file_path</th>\n",
       "      <th>season</th>\n",
       "      <th>show_name</th>\n",
       "      <th>lemmatize_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Seinfeld Chronicles</td>\n",
       "      <td>Seinfeld_Episodes/Season_1/S01_E01_The_Seinfel...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>so im on line at the supermarket two woman in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The Robbery</td>\n",
       "      <td>Seinfeld_Episodes/Season_1/S01_E02_The_Robbery...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>so i move into the centre lane now i get ahead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Male Unbonding</td>\n",
       "      <td>Seinfeld_Episodes/Season_1/S01_E03_Male_Unbond...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>most men like working on thing tool object fix...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The Stock Tip</td>\n",
       "      <td>Seinfeld_Episodes/Season_1/S01_E04_The_Stock_T...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>went out to dinner the other night check came ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The Mango</td>\n",
       "      <td>Seinfeld_Episodes/Season_5/S05_E01_The_Mango.txt</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>a female orgasm is kinda like the bat cave a v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode             episode_name  \\\n",
       "0        1  The Seinfeld Chronicles   \n",
       "1        2              The Robbery   \n",
       "2        3           Male Unbonding   \n",
       "3        4            The Stock Tip   \n",
       "4        1                The Mango   \n",
       "\n",
       "                                           file_path  season  show_name  \\\n",
       "0  Seinfeld_Episodes/Season_1/S01_E01_The_Seinfel...       1          1   \n",
       "1  Seinfeld_Episodes/Season_1/S01_E02_The_Robbery...       1          1   \n",
       "2  Seinfeld_Episodes/Season_1/S01_E03_Male_Unbond...       1          1   \n",
       "3  Seinfeld_Episodes/Season_1/S01_E04_The_Stock_T...       1          1   \n",
       "4   Seinfeld_Episodes/Season_5/S05_E01_The_Mango.txt       5          1   \n",
       "\n",
       "                                      lemmatize_text  \n",
       "0  so im on line at the supermarket two woman in ...  \n",
       "1  so i move into the centre lane now i get ahead...  \n",
       "2  most men like working on thing tool object fix...  \n",
       "3  went out to dinner the other night check came ...  \n",
       "4  a female orgasm is kinda like the bat cave a v...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split The Data Frame\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.lemmatize_text, df.show_name, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('count_vectorizer', CountVectorizer()), \n",
    "                     ('tfidf_vectorizer', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('count_vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       " ...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Curb       0.86      1.00      0.92         6\n",
      "    Seinfeld       1.00      0.83      0.91         6\n",
      "\n",
      "   micro avg       0.92      0.92      0.92        12\n",
      "   macro avg       0.93      0.92      0.92        12\n",
      "weighted avg       0.93      0.92      0.92        12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, test_predictions, \n",
    "                              target_names = le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
