{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) Process For Seinfeld Transcripts\n",
    "\n",
    "This notebook will outline the process of cleaning, tokenizing, and vectorizing text transcripts of Seinfeld Season 5 Episodes. Source of transcripts: https://www.seinfeldscripts.com/seinfeld-scripts.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Alex/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import the NLTK library, tokenizer, and methods\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read The Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seinfeld_directory = 'Seinfeld_Episodes/Season_5/'\n",
    "\n",
    "seinfeld_season_5_episodes = ['S05_E01_The_Mango.txt', 'S05_E02_The_Puffy_Shirt.txt',\n",
    "                              'S05_E03_The_Glasses.txt', 'S05_E04_The_Sniffing_Accountant.txt',\n",
    "                              'S05_E05_The_Bris.txt', 'S05_E06_The_Lip_Reader.txt',\n",
    "                              'S05_E07_The_Non_Fat_Yogurt.txt', 'S05_E08_The_Barber.txt',\n",
    "                              'S05_E09_The_Masseuse.txt', 'S05_E10_The_Cigar_Store_Indian.txt',\n",
    "                              'S05_E11_The_Conversion.txt', 'S05_E12_The_Stall.txt',\n",
    "                              'S05_E13_The_Dinner_Party.txt', 'S05_E14_The_Marine_Biologist.txt',\n",
    "                              'S05_E15_The_Pie.txt', 'S05_E16_The_Stand-In.txt',\n",
    "                              'S05_E17_The_Wife.txt', 'S05_E18_The_Raincoats_Part_1.txt',\n",
    "                              'S05_E19_The_Raincoats_Part_2.txt', 'S05_E20_The_Fire.txt',\n",
    "                              'S05_E21_The_Hamptons.txt', 'S05_E22_The Opposite.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(seinfeld_directory + seinfeld_season_5_episodes[1], 'r') as file:\n",
    "    raw_text_episode_2 = file.read().replace('\\n', ' ')\n",
    "    #.replace('[','(').replace(']',')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Setting: Jerry's apartment] (Jerry and George are waiting for Kramer, so he can help them move George's stuff back into his parent's house) GEORGE: I can't believe this! JERRY: Oh, it won't be for that long. GEORGE: How can I do this?! How can I move back in with those people? Please, tell me! They're insane! You know that. JERRY: Hey, my parents are just as crazy as your parents. GEORGE: How can you compare you parents to my parents?! JERRY: My father has never thrown anything out. Ever! GEORGE: My father wears his sneakers in the pool! Sneakers! JERRY: My mother has never set foot in a natural body of water. GEORGE: (Showing Jerry up) Listen carefully. My mother has never laughed. Ever. Not a giggle, not a chuckle, not a tee-hee.. never went 'Ha!' JERRY: A smirk? GEORGE: Maybe!.. And I'm moving back in there! JERRY: I told you I'd lend you the money for the rent. GEORGE: No, no, no, no. Borrowing money from a friend is like having sex. It just completely changes the relationship. (Kramer stumbles in) KRAMER: Alright. I'm ready. (To\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text_episode_2[:1050]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning The Text Data\n",
    "\n",
    "#Step 1: Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(raw_text):\n",
    "    raw_text_quotes = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", raw_text)\n",
    "    cleaned_text = []\n",
    "    for word in raw_text_quotes.split(\" \"):\n",
    "        if not \":\" in word:\n",
    "            for symbol in \".,?!'\":\n",
    "                word = word.replace(symbol, '').lower()\n",
    "            #Checks for blank elements\n",
    "            if word:\n",
    "                cleaned_text.append(word)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i cant believe this oh it wont be for that long how can i do this how can i move back in with those people please tell me theyre insane you know that hey my parents are just as crazy as your parents how can you compare you parents to my parents my father has never thrown anything out ever my father wears his sneakers in the pool sneakers my mother has never set foot in a natural body of water listen carefully my mother has never laughed ever not a giggle not a chuckle not a tee-hee never went ha a smirk maybe and im moving back in there i told you id lend you the money for the rent no no no no borrowing money from a friend is like having sex it just completely changes the relationship alright im ready you know i still dont understand - why do you want to move back in with your parents i dont want to im outta money i got 714 dollars left in the bank well move in here whats that why doesnt he just move in here yeah yeah im gonna move in with him he doesnt even'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text_episode_2 = clean_text(raw_text_episode_2)\n",
    "' '.join(clean_text_episode_2[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize The Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uses the NLP tokenize method to tokenize the script\n",
    "\n",
    "def tokenize(cleaned_text):\n",
    "    joined_sentence = ' '.join(cleaned_text)\n",
    "    tokenized_sentence = word_tokenize(joined_sentence)\n",
    "    \n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i cant believe this oh it wont be for that long how can i do this how can i move back in with those people please tell me theyre insane you know that hey my parents are just as crazy as your parents how can you compare you parents to my parents my father has never thrown anything out ever my father wears his sneakers in the pool sneakers my mother has never set foot in a natural body of water listen carefully my mother has never laughed ever not a giggle not a chuckle not a tee-hee never went ha a smirk maybe and im moving back in there i told you id lend you the money for the rent no no no no borrowing money from a friend is like having sex it just completely changes the relationship alright im ready you know i still dont understand - why do you want to move back in with your parents i dont want to im outta money i got 714 dollars left in the bank well move in here whats that why doesnt he just move in here yeah yeah im gon na move in with him he doesnt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text_episode_2 = tokenize(clean_text_episode_2)\n",
    "' '.join(tokenized_text_episode_2[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "\n",
    "## Method 1: Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 1, 'security': 1, 'session': 1, 'take': 7, 'guys-': 1, 'whos': 1, 'girlfriend': 1, 'important': 1, 'kind': 2, 'call': 1, 'trend': 1, 'fine': 1, 'insane': 1, 'comparing': 1, 'anything': 5, 'sandwiches': 3, 'i': 90, 'his': 10, 'bop': 1, 'volunteer': 1, 'emory': 1, 'jell-o': 2, 'wha-': 1, 'utensils': 1, 'interested': 1, 'youve': 2, 'couldve': 1, 'alright': 5, 'let': 6, 'someone': 1, 'else': 1, 'myself': 1, 'ugh': 1, 'provides': 1, 'wear': 13, 'looks': 2, 'factory': 1, 'do': 14, 'was': 14, 'water': 1, 'board': 1, 'those': 4, 'ta': 4, 'damaged': 1, 'easy': 1, 'right': 5, 'collection': 2, 'but': 10, 'department': 1, 'overuse': 1, 'any': 4, 'are': 19, '19th': 1, 'on': 16, 'wants': 1, 'hadnt': 1, 'georgie': 3, 'work': 4, 'lets': 3, 'bryant': 2, 'pair': 1, 'show': 5, 'have': 11, 'chuckle': 1, 'speak': 1, 'didya': 1, 'well': 13, 'time': 2, 'jersey': 1, 'creamy': 1, 'a-a-a-a-a': 1, 'men': 1, 'young': 1, 'eh': 1, 'father': 3, 'me': 22, 'meet': 1, 'life': 1, 'know': 19, 'taking': 1, 'people': 6, 'how': 9, 'elaine': 2, 'became': 3, 'lot': 2, 'seinfeld': 1, 'just': 10, 'hi': 1, 'hair': 1, 'made': 1, 'hands': 18, 'most': 1, 'helped': 1, 'she': 6, 'kidding': 1, 'successor': 1, 'first': 1, 'from': 3, 'has': 5, 'nah': 1, 'were': 4, 'look': 14, 'for': 10, 'building': 2, 'one': 8, 'body': 1, 'nut': 1, 'traveled': 1, 'did': 3, 'york': 1, 'more': 6, 'condition': 1, 'yep': 6, 'deformed': 1, 'mens': 1, 'knew': 3, 'making': 1, 'mentioned': 2, 'fantastic': 1, 'careful': 3, 'another': 1, 'word': 2, 'card': 1, 'see': 3, 'worried': 1, 'god': 2, 'nodding': 1, 'it': 36, 'goodwill': 2, 'very': 4, 'course': 1, 'producing': 1, 'funny': 1, 'seeking': 1, 'they': 4, 'can': 6, 'swing': 1, 'them': 11, 'guess': 2, 'comedian': 1, 'side': 1, '18': 1, 'mind': 1, 'because': 2, 'mrs': 2, 'supposed': 1, 'benefit': 4, 'ready': 1, 'story': 1, 'cool': 1, 'poor': 2, 'this': 25, 'end': 1, 'hell': 1, 'worry': 1, 'beauty': 1, 'changes': 1, 'great': 2, 'national': 1, 'eats': 1, 'get': 6, 'close': 1, 'out': 9, 'ironed': 1, 'delicate': 1, 'civil': 4, 'stuff': 2, 'masculine': 1, 'perfect': 1, 'compare': 1, 'money': 4, 'parents': 6, 'become': 1, 'saw': 1, 'none': 1, 'mean': 3, 'we': 4, '-': 21, 'into': 2, 'suitcases': 1, 'pool': 1, 'ohh': 1, 'orders': 2, 'been': 1, 'locked': 1, 'labor': 1, 'tragic': 1, 'true': 2, 'move': 7, 'health': 1, 'even': 2, 'television': 1, 'that': 28, 'bathroom': 2, 'before': 2, 'contest': 1, 'im': 13, 'truth': 1, 'relationship': 1, 'spend': 1, 'knuckles': 1, 'photo': 1, 'outta': 1, 'room': 1, 'natural': 1, 'why': 8, 'ayye': 1, 'lowtalker': 1, 'wha': 1, 'need': 1, 'noticed': 1, 'bananas': 3, 'heres': 1, 'coulda': 1, 'world': 2, 'mitts': 1, 'hope': 2, 'compassionate': 1, 'cant': 7, 'pass': 1, 'whyd': 1, 'claw': 1, 'touched': 1, 'over': 2, 'kramer': 6, 'had': 5, 'will': 3, 'dont': 19, 'dependent': 1, 'only': 2, 'jerrys': 1, 'the': 70, 'offer': 1, 'white': 4, 'jerry': 8, 'pirates': 2, 'spilled': 1, 'epidermis': 1, 'happened': 2, 'na': 9, 'phobia': 1, 'captain': 1, 'if': 2, 'five': 2, 'door': 2, 'extraordinary': 1, 'hello': 2, 'feel': 1, 'exquisite': 2, 'want': 11, 'domain': 1, 'talking': 5, 'muscles': 1, 'bickering': 1, 'bologna': 4, 'off': 1, 'thanks': 2, 'ok': 1, 'told': 1, 'should': 1, 'office': 1, 'industries': 1, 'thing': 1, 'um': 1, 'when': 10, 'better': 2, 'ridiculous': 2, 'about': 13, 'sex': 1, 'never': 8, 'point': 1, 'try': 1, 'morning': 2, 'quite': 1, 'at': 10, 'said': 8, 'minimum': 1, 'comes': 1, 'swamis': 1, 'frozen': 1, 'costanza': 2, 'whatdya': 1, 'other': 1, 'proportion': 1, 'college': 1, 'air': 1, 'theyre': 8, 'listen': 1, 'stress': 1, 'doing': 1, 'use': 2, 'new': 6, 'remember': 1, 'won': 1, 'ya': 2, 'manipulate': 1, 'completely': 1, 'left': 2, 'unusual': 1, 'make': 1, 'as': 3, 'all': 13, 'big': 1, 'long': 1, 'likes': 1, 'worth': 1, 'one-in-a-million': 1, 'mckigneys': 1, 'head': 1, 'here': 6, 'he': 14, 'hmm': 1, 'help': 2, 'week': 1, 'looking': 1, 'asked': 2, 'some': 5, 'little': 2, 'happening': 1, '``': 2, 'might': 2, 'comfortable': 1, 'silver': 3, 'maybe': 3, 'organization': 1, 'dressed': 1, 'tv': 4, 'ha': 1, 'nice': 1, 'us': 1, 'where': 4, 'up': 7, 'or': 1, 'today': 5, 'beautiful': 1, 'set': 3, 'uhh': 1, 'whole': 2, 'restaurant': 1, 'sorry': 4, 'speaking': 1, 'a': 54, 'agreed': 3, 'agree': 1, 'position': 1, 'getting': 2, 'wanted': 1, 'based': 2, 'him': 5, 'monte': 1, 'bag': 1, 'speaks': 1, 'months': 1, 'crazy': 4, 'neither': 1, 'yes': 10, 'thrown': 1, 'stores': 2, 'seen': 5, 'damaging': 1, 'match': 1, 'laughed': 1, 'clothing': 3, 'your': 11, 'still': 1, 'cub': 1, 'pirate': 4, 'giggle': 1, 'homeless': 2, 'you': 87, 'is': 17, 'lend': 1, 'wearing': 6, 'whats': 2, 'master': 1, 'talent': 1, 'scouts': 1, 'business': 1, 'no': 16, 'yeah': 25, '90s': 1, 'much': 2, 'unable': 1, 'minutes': 2, 'soft': 2, 'self-control': 1, 'and': 21, 'car': 1, 'hear': 4, 'mckigney': 1, 'really': 4, 'in': 29, 'bring': 2, 'devil-may-care': 1, 'not': 11, 'bank': 1, 'puffed': 2, 'always': 4, 'designer': 1, '714': 1, 'boutiques': 1, 'eat': 2, 'now': 7, 'ever': 7, 'hes': 2, 'afraid': 1, 'am': 3, 'tell': 5, 'her': 2, 'cares': 1, 'already': 1, 'avoiding': 1, 'service': 5, 'model': 2, 'ray': 3, 'tonight': 1, 'sees': 1, 'gon': 9, 'went': 4, 'later': 2, 'like': 10, 'fashions': 1, 'makin': 1, 'sure': 1, 'since': 2, 'joke': 1, 'stocking': 1, 'sandwich': 1, 'love': 1, 'asking': 1, 'dinner': 3, 'youd': 1, 'done': 2, 'nobody': 1, 'terrific': 1, 'nothing': 2, 'promoting': 2, 'audience': 1, 'low-talkers': 1, 'of': 18, 'performers': 1, 'person': 1, 'stop': 2, 'chandelier': 1, 'give': 2, 'milky': 4, 'thank': 4, 'uh': 7, 'every': 1, 'until': 1, 'my': 18, 'excuse': 2, 'terribly': 1, 'scissor': 1, 'there': 6, 'home': 1, 'plenty': 1, 'going': 10, 'woman': 6, 'painted': 1, 'post': 1, 'fact': 1, 'manual': 1, 'handing': 1, 'pizza': 1, 'id': 3, 'clothe': 2, 'acupuncturists': 1, 'conversation': 1, 'be': 17, 'good': 2, 'may': 1, 'wears': 1, 'back': 5, 'couldnt': 5, 'carefully': 1, 'thinking': 1, 'whyre': 1, 'foot': 1, 'uh-huh': 2, 'job': 2, 'george': 6, 'think': 4, 'tip-top': 1, 'yet': 2, 'an': 2, 'jobs': 1, 'having': 1, 'pay': 1, 'oven': 1, 'so': 11, 'ill': 5, 'glad': 1, 'find': 1, 'shes': 6, 'sitting': 1, 'with': 15, 'please': 4, 'hey': 6, 'anymore': 1, 'graduate': 1, 'eventually': 1, 'believe': 4, 'theres': 2, 'ask': 1, 'tee-hee': 1, 'saying': 2, 'frank': 1, 'keep': 1, 'herbalists': 1, 'who': 2, 'remind': 1, 'moving': 1, 'put': 1, 'giddy-up': 1, 'strained': 1, 'talk': 3, 'toy': 1, 'feed': 1, 'own': 3, 'what': 32, 'down': 1, 'place': 1, 'come': 5, 'test': 4, 'pie': 1, 'shirt': 12, 'dollar': 3, 'mailman': 1, 'smooth': 1, 'doesnt': 3, 'smirk': 1, 'wont': 1, 'modeling': 2, 'than': 2, 'touch': 1, 'used': 3, 'hand': 10, 'mention': 1, 'everybodys': 1, 'oh': 14, 'indigent': 2, 'facing': 1, 'waiter': 1, 'hears': 1, 'something': 1, 'puffy': 11, 'towards': 1, 'sneakers': 2, 'tuesday': 1, 'understand': 3, 'needy': 1, 'toilet': 1, 'cristo': 1, 'bet': 1, 'to': 58, 'rent': 1, 'idea': 1, 'youre': 14, 'leslie': 2, 'ive': 3, 'friday': 1, 'go': 4, 'would': 4, 'okay': 3, 'aye': 1, 'dollars': 1, 'mother': 3, 'constant': 1, 'friend': 1, 'kinda': 2, 'its': 11, 'check': 1, 'shape': 1, 'night': 1, 'borrowing': 1, 'gumbel': 1, \"''\": 2, 'got': 13, 'these': 6, 'cure': 1, 'mmm': 1, 'two': 3, 'thats': 8, 'didnt': 3, 'could': 2}\n"
     ]
    }
   ],
   "source": [
    "def count_vectorize(episode, vocab=None):\n",
    "    if vocab:\n",
    "        unique_words = vocab\n",
    "    else:\n",
    "        unique_words = list(set(episode))\n",
    "    \n",
    "    episode_dict = {i:0 for i in unique_words}\n",
    "    \n",
    "    for word in episode:\n",
    "        episode_dict[word] += 1\n",
    "    \n",
    "    return episode_dict\n",
    "\n",
    "vectorized_episode_2 = count_vectorize(tokenized_text_episode_2)\n",
    "print(vectorized_episode_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Vectorization with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(BoW_dict):\n",
    "    total_word_count = sum(BoW_dict.values())\n",
    "    \n",
    "    for ind, val in BoW_dict.items():\n",
    "        BoW_dict[ind] = val/ total_word_count\n",
    "    \n",
    "    return BoW_dict\n",
    "\n",
    "episode_2_term_frequency = term_frequency(vectorized_episode_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i  : 0.04070556309362291  \n",
      "you  : 0.03934871099050214  \n",
      "the  : 0.031659882406151155  \n",
      "to  : 0.026232473993668095  \n",
      "a  : 0.024423337856173746  \n",
      "it  : 0.016282225237449162  \n",
      "what  : 0.014473089099954812  \n",
      "in  : 0.013116236996834047  \n",
      "that  : 0.012663952962460461  \n",
      "this  : 0.011307100859339697  \n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for i in Counter(episode_2_term_frequency).most_common(10): \n",
    "    print(i[0],\" :\",i[1],\" \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_document_frequency(list_of_dicts):\n",
    "    vocab_set = set()\n",
    "    # Iterate through list of dfs and add index to vocab_set\n",
    "    for d in list_of_dicts:\n",
    "        for word in d.keys():\n",
    "            vocab_set.add(word)\n",
    "    \n",
    "    # Once vocab set is complete, create an empty dictionary with a key for each word and value of 0.\n",
    "    full_vocab_dict = {i:0 for i in vocab_set}\n",
    "    \n",
    "    # Loop through each word in full_vocab_dict\n",
    "    for word, val in full_vocab_dict.items():\n",
    "        docs = 0\n",
    "        \n",
    "        # Loop through list of dicts.  Each time a dictionary contains the word, increment docs by 1\n",
    "        for d in list_of_dicts:\n",
    "            if word in d:\n",
    "                docs += 1\n",
    "        \n",
    "        # Now that we know denominator for equation, compute and set IDF value for word\n",
    "        \n",
    "        full_vocab_dict[word] = np.log((len(list_of_dicts)/ float(docs)))\n",
    "    \n",
    "    return full_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(list_of_dicts):\n",
    "    # Create empty dictionary containing full vocabulary of entire corpus\n",
    "    doc_tf_idf = {}\n",
    "    idf = inverse_document_frequency(list_of_dicts)\n",
    "    full_vocab_list = {i:0 for i in list(idf.keys())}\n",
    "    \n",
    "    # Create tf-idf list of dictionaries, containing a dictionary that will be updated for each document\n",
    "    tf_idf_list_of_dicts = []\n",
    "    \n",
    "    # Now, compute tf and then use this to compute and set tf-idf values for each document\n",
    "    for doc in list_of_dicts:\n",
    "        doc_tf = term_frequency(doc)\n",
    "        for word in doc_tf:\n",
    "            doc_tf_idf[word] = doc_tf[word] * idf[word]\n",
    "        tf_idf_list_of_dicts.append(doc_tf_idf)\n",
    "    \n",
    "    return tf_idf_list_of_dicts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
