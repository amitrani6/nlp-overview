{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(file_path):\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        raw_text = file.read().replace('\\n', ' ')\n",
    "    \n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "# It is generally a good idea to also remove punctuation\n",
    "import string\n",
    "\n",
    "# Now we have a list that includes all english stopwords, as well as all punctuation\n",
    "stopwords_list += list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_stones_article = open_file(\"sample_text/December's Children (And Everybody's).txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(rolling_stones_article)\n",
    "\n",
    "# It is usually a good idea to lowercase all tokens during this step, as well\n",
    "stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['december',\n",
       " \"'s\",\n",
       " 'children',\n",
       " 'and',\n",
       " 'everybody',\n",
       " \"'s\",\n",
       " 'december',\n",
       " \"'s\",\n",
       " 'children',\n",
       " 'and',\n",
       " 'everybody',\n",
       " \"'s\",\n",
       " 'fifth',\n",
       " 'american',\n",
       " 'studio',\n",
       " 'album',\n",
       " 'english',\n",
       " 'rock',\n",
       " 'band',\n",
       " 'rolling',\n",
       " 'stones',\n",
       " 'released',\n",
       " 'december',\n",
       " '1965',\n",
       " 'although',\n",
       " 'largely',\n",
       " 'draws',\n",
       " 'songs',\n",
       " 'issued',\n",
       " 'earlier',\n",
       " 'year',\n",
       " 'united',\n",
       " 'kingdom',\n",
       " 'album',\n",
       " 'includes',\n",
       " 'three',\n",
       " 'previously',\n",
       " 'unreleased',\n",
       " 'tunes',\n",
       " 'it',\n",
       " 'last',\n",
       " 'group',\n",
       " \"'s\",\n",
       " 'early',\n",
       " 'albums',\n",
       " 'feature',\n",
       " 'numerous',\n",
       " 'cover',\n",
       " 'songs',\n",
       " 'writers',\n",
       " 'mick',\n",
       " 'jagger',\n",
       " 'keith',\n",
       " 'richards',\n",
       " 'wrote',\n",
       " 'half',\n",
       " 'songs',\n",
       " 'there',\n",
       " 'sessions',\n",
       " 'record',\n",
       " 'album',\n",
       " 'many',\n",
       " 'songs',\n",
       " 'drawn',\n",
       " 'sessions',\n",
       " 'uk',\n",
       " 'edition',\n",
       " 'out',\n",
       " 'our',\n",
       " 'heads',\n",
       " 'september',\n",
       " '1965',\n",
       " 'los',\n",
       " 'angeles',\n",
       " 'many',\n",
       " 'tracks',\n",
       " 'appeared',\n",
       " 'earlier',\n",
       " 'uk',\n",
       " 'versions',\n",
       " 'rolling',\n",
       " 'stones',\n",
       " 'albums',\n",
       " 'left',\n",
       " 'american',\n",
       " 'counterparts',\n",
       " 'other',\n",
       " 'tracks',\n",
       " 'unreleased',\n",
       " 'tracks',\n",
       " 'recorded',\n",
       " 'recording',\n",
       " 'sessions',\n",
       " 'singles-only',\n",
       " 'releases',\n",
       " 'joining',\n",
       " 'core',\n",
       " 'members',\n",
       " 'mick',\n",
       " 'jagger',\n",
       " 'vocals',\n",
       " 'brian',\n",
       " 'jones',\n",
       " 'guitars',\n",
       " 'keith',\n",
       " 'richards',\n",
       " 'guitars',\n",
       " 'bill',\n",
       " 'wyman',\n",
       " 'bass',\n",
       " 'charlie',\n",
       " 'watts',\n",
       " 'drums',\n",
       " 'former',\n",
       " 'stones',\n",
       " 'member',\n",
       " 'ian',\n",
       " 'stewart',\n",
       " 'piano',\n",
       " 'as',\n",
       " 'early',\n",
       " 'albums',\n",
       " 'produced',\n",
       " 'band',\n",
       " 'manager',\n",
       " 'andrew',\n",
       " 'loog',\n",
       " 'oldham',\n",
       " 'december',\n",
       " \"'s\",\n",
       " 'children',\n",
       " 'and',\n",
       " 'everybody',\n",
       " \"'s\",\n",
       " 'reached',\n",
       " 'no',\n",
       " '4',\n",
       " 'us',\n",
       " 'certified',\n",
       " '``',\n",
       " 'gold',\n",
       " \"''\",\n",
       " '1',\n",
       " 'bassist',\n",
       " 'bill',\n",
       " 'wyman',\n",
       " 'quotes',\n",
       " 'jagger',\n",
       " '1968',\n",
       " 'calling',\n",
       " 'record',\n",
       " '``',\n",
       " 'album',\n",
       " \"'s\",\n",
       " 'collection',\n",
       " 'songs',\n",
       " \"''\",\n",
       " 'accordingly',\n",
       " 'briefly',\n",
       " 'detailed',\n",
       " 'wyman',\n",
       " \"'s\",\n",
       " 'otherwise',\n",
       " 'exhaustive',\n",
       " 'book',\n",
       " 'rolling',\n",
       " 'stones',\n",
       " 'the',\n",
       " 'group',\n",
       " \"'s\",\n",
       " 'second',\n",
       " 'us',\n",
       " 'no',\n",
       " '1',\n",
       " 'get',\n",
       " 'off',\n",
       " 'my',\n",
       " 'cloud',\n",
       " 'highest-charting',\n",
       " 'single',\n",
       " 'album',\n",
       " 'also',\n",
       " 'major',\n",
       " 'chart',\n",
       " 'topper',\n",
       " 'band',\n",
       " \"'s\",\n",
       " 'native',\n",
       " 'uk',\n",
       " 'several',\n",
       " 'markets',\n",
       " 'in',\n",
       " 'august',\n",
       " '2002',\n",
       " 'album',\n",
       " 'reissued',\n",
       " 'new',\n",
       " 'remastered',\n",
       " 'cd',\n",
       " 'sacd',\n",
       " 'digipak',\n",
       " 'abkco',\n",
       " 'records',\n",
       " '``',\n",
       " 'look',\n",
       " 'what',\n",
       " 'you',\n",
       " \"'ve\",\n",
       " 'done',\n",
       " \"''\",\n",
       " 'remains',\n",
       " 'album',\n",
       " \"'s\",\n",
       " 'cut',\n",
       " 'issued',\n",
       " 'true',\n",
       " 'stereo',\n",
       " 'the',\n",
       " 'title',\n",
       " 'album',\n",
       " 'came',\n",
       " 'band',\n",
       " \"'s\",\n",
       " 'manager',\n",
       " 'andrew',\n",
       " 'loog',\n",
       " 'oldham',\n",
       " 'facetiously',\n",
       " 'credits',\n",
       " '``',\n",
       " 'lou',\n",
       " 'folk-rock',\n",
       " 'adler',\n",
       " \"''\",\n",
       " 'liner',\n",
       " 'notes',\n",
       " 'back',\n",
       " 'cover',\n",
       " 'according',\n",
       " 'jagger',\n",
       " 'oldham',\n",
       " \"'s\",\n",
       " 'idea',\n",
       " 'hip',\n",
       " 'beat',\n",
       " 'poetry',\n",
       " '2',\n",
       " 'the',\n",
       " 'front',\n",
       " 'cover',\n",
       " 'photo',\n",
       " 'band',\n",
       " 'gered',\n",
       " 'mankowitz',\n",
       " 'previously',\n",
       " 'used',\n",
       " 'uk',\n",
       " 'edition',\n",
       " 'out',\n",
       " 'our',\n",
       " 'heads']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopped_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated cleaning function with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seinfeld_pilot = 'Seinfeld_Episodes/Season_1/S01_E01_The_Seinfeld_Chronicles.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seinfeld_pilot_raw_text = open_file(seinfeld_pilot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_cleaned_episode(raw_text, stop_words = False):\n",
    "    \n",
    "    raw_text_no_notes = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", raw_text)\n",
    "\n",
    "    for symbol in \"*,#-.?!''\\n\":\n",
    "        raw_text_no_notes = raw_text_no_notes.replace(symbol, '').lower()\n",
    "  \n",
    "    cleaned_text = raw_text_no_notes.split(\" \")    \n",
    "    \n",
    "    for i in cleaned_text:\n",
    "        \n",
    "        if i.endswith(':') == True or i == '' or i == ' ':\n",
    "            cleaned_text.remove(i)\n",
    "            \n",
    "        i = i.replace('.', '')\n",
    "        i = i.replace('?', '')\n",
    "        i = i.replace('!', '')\n",
    "        \n",
    "    cleaned_text  = [word for word in cleaned_text if word.endswith(':') == False]\n",
    "    \n",
    "    if stop_words:\n",
    "        \n",
    "        cleaned_text  = [word for word in cleaned_text if word.lower() not in stop_words]\n",
    "     \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so', 'im', 'on', 'line', 'at', 'the', 'supermarket', 'two', 'women', 'in']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_cleaned_episode(seinfeld_pilot_raw_text, stop_words = False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK's stopwords list\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import NLTK's punctuation list\n",
    "# '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "import string\n",
    "\n",
    "# Import NLTK's word tokenizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Add only English language stopwords and punctuation list together\n",
    "nltk_stopwords_list = stopwords.words('english')\n",
    "nltk_stopwords_list += list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_episode(raw_text, custom_stop_words = False):\n",
    "    \n",
    "    # Copy the NLTK list in case of customization\n",
    "    stop_words_list = nltk_stopwords_list\n",
    "    \n",
    "    # Removes all text between and including brackets and parenthesis with RegEx\n",
    "    raw_text_no_stage_notes = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", raw_text)\n",
    "\n",
    "    # Remove all text with colons (:), i.e. character line indications\n",
    "    raw_text_no_stage_notes_or_names = raw_text_no_stage_notes.split(\" \")\n",
    "    \n",
    "    for i in raw_text_no_stage_notes_or_names:\n",
    "        \n",
    "        if i.endswith(':') == True or i == '' or i == ' ':\n",
    "            raw_text_no_stage_notes_or_names.remove(i)\n",
    "            \n",
    "    # Rejoin all of the text as one string for tokenization\n",
    "    raw_text_rejoined = \" \".join(raw_text_no_stage_notes_or_names)\n",
    "    \n",
    "    \n",
    "    # If a list of additional custom stopwords are passed add them to the default\n",
    "    # NLTK stopwords and punctuation list\n",
    "    if custom_stop_words:\n",
    "        \n",
    "        stop_words_list += custom_stop_words\n",
    "    \n",
    "    # Tokenize the raw text\n",
    "    token_list = word_tokenize(raw_text_rejoined)\n",
    "    \n",
    "    # Remove stop words and punctuation\n",
    "    cleaned_and_tokenized_list = [w.lower() for w in token_list if w not in stop_words_list]\n",
    "\n",
    "    return cleaned_and_tokenized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'line',\n",
       " 'supermarket',\n",
       " 'two',\n",
       " 'women',\n",
       " 'front',\n",
       " 'one',\n",
       " 'total']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_episode(seinfeld_pilot_raw_text, custom_stop_words = False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
